# yaml-language-server: $schema=values.schema.json
---
## Host configuration
##
host:
  ## Host timezone
  ## e.g. 'Europe/Berlin'
  ##
  tz: Europe/Berlin
  ## Host user's ID
  ## Can be obtained via `id -u`
  ##
  uid: -1
  ## Host user's group ID
  ## Can be obtained via `id -g`
  ##
  gid: -1
## cert-manager chart configuration
## see available values at https://artifacthub.io/packages/helm/cert-manager/cert-manager#configuration
##
cert-manager:
  namespace: cert-manager
  crds:
    enabled: true
    keep: true
  config:
    apiVersion: controller.config.cert-manager.io/v1alpha1
    kind: ControllerConfiguration
    logging:
      verbosity: 2
      format: text
    # DO NOT disable this option, otherwise cert-manager won't be able to create a certificate
    enableGatewayAPI: true
## Additional configuration for cert-manager tailored to this chart
##
x-cert-manager:
  ## ClusterIssuer configuration
  ## This issuer will be used to automatically issue TLS certificates for Ingress/Gateway API
  ##
  cluster-issuer:
    ## ACME server URL
    ## By default points to the Let's Encrypt's prod environment
    ##
    ## If you're having issues, you can switch to the staging environment to not hit the rate limits
    ## Staging URL: https://acme-staging-v02.api.letsencrypt.org/directory
    ##
    ## DO NOT change this option unless you know what you're doing
    server: https://acme-v02.api.letsencrypt.org/directory
    # Your email to register an account with Let's Encrypt
    email: "" # REQUIRED
    ## Solvers configuration
    ##
    ## Cloudflare is used by default with implication that you've transferred your domain to Cloudflare.
    ##
    ## Configuring the Cloudflare solver is super easy:
    ## 1. Go to the Cloudflare Dashboard: User Profile > API Tokens > API Tokens.
    ## 2. Specify the permissions for the token as follows:
    ##    Permissions:
    ##      Zone - DNS - Edit
    ##      Zone - Zone - Read
    ##    Zone Resources:
    ##      Include - All Zones
    ## 3. Create a secret in the `cert-manager` namespace with the name `cloudflare-api-key-secret`
    ##
    ## If you want to use a different provider, please read docs on DNS01 validation and supported providers:
    ## https://cert-manager.io/docs/configuration/acme/dns01/
    ## NOTE: The only valid solver is a DNS01 solver, since only it can issue wildcard certs
    solvers:
      - dns01:
          cloudflare:
            apiTokenSecretRef:
              name: cloudflare-api-key-secret
              key: api-key
## Authentication provider to use
##
authProvider: authentik
## Authentik chart configuration
## See available values at https://artifacthub.io/packages/helm/goauthentik/authentik#values
##
authentik:
  ## Namespace to deploy authentik resources to
  ##
  namespace: authentik
  authentik:
    enabled: true
    secret_key: file:///secrets/authentik/secret-key
    error_reporting:
      enabled: false
    postgresql:
      user: authentik
      password: file:///secrets/postgres/user-password
  server:
    metrics:
      enabled: false
    volumes:
      - name: secret-key-creds
        secret:
          secretName: authentik-secret-key
      - name: postgres-creds
        secret:
          secretName: authentik-postgres-credentials
    volumeMounts:
      - name: secret-key-creds
        mountPath: /secrets/authentik
        readOnly: true
      - name: postgres-creds
        mountPath: /secrets/postgres
        readOnly: true
  worker:
    volumes:
      - name: secret-key-creds
        secret:
          secretName: authentik-secret-key
      - name: postgres-creds
        secret:
          secretName: authentik-postgres-credentials
    volumeMounts:
      - name: secret-key-creds
        mountPath: /secrets/authentik
        readOnly: true
      - name: postgres-creds
        mountPath: /secrets/postgres
        readOnly: true
  # service account is required by worker to create a managed embedded outpost
  # DO NOT DISABLE
  serviceAccount:
    create: true
  # postgresql bitnami subchart
  # https://artifacthub.io/packages/helm/bitnami/postgresql#parameters
  postgresql:
    enabled: true
    auth:
      username: authentik
      database: authentik
      existingSecret: authentik-postgres-credentials
      secretKeys:
        adminPasswordKey: admin-password
        userPasswordKey: user-password
      usePasswordFiles: true
    backup:
      enabled: true
      cronjob:
        schedule: "0 5 * * *"
        concurrencyPolicy: Replace
        storage:
          # disable PVC, we'll save backups to a local dir
          enabled: false
          mountPath: /backup/pgdump
        extraVolumes:
          - name: backup-dir
            hostPath:
              path: /opt/authentik/postgresql/backup
              type: Directory
        extraVolumeMounts:
          - name: backup-dir
            mountPath: /backup/pgdump
  # redis bitnami subchart
  redis:
    enabled: true
x-authelia:
  ## Namespace to deploy Authelia resources to
  ##
  namespace: authelia
## Authelia chart configuration
## See available values at https://github.com/authelia/chartrepo/blob/master/charts/authelia/values.yaml
##
## The chart is currently considered beta status, and as such is subject to breaking changes.
##
authelia:
  ## Pod configuration
  ##
  pod:
    kind: DaemonSet
    replicas: 1
    annotations: {}
    labels: {}
    resources: {}
    # securityContext:
    #   container:
    #     runAsNonRoot: true
    #     runAsUser: 1000
    #     runAsGroup: 1000
    #     privileged: false
    #     readOnlyRootFilesystem: false
    #     allowPrivilegeEscalation: false
    #   pod:
    #     fsGroup: 1000
    env:
      - name: TZ
        value: "{{ .Values.host.tz }}"
        ## Enable template functions
        ## See https://www.authelia.com/reference/guides/templating/
        ##
      - name: AUTHELIA_SESSION_SECRET_FILE
        value: /etc/secrets/session-secret
      - name: AUTHELIA_STORAGE_ENCRYPTION_KEY_FILE
        value: /etc/secrets/storage-encryption-key
      - name: AUTHELIA_IDENTITY_VALIDATION_RESET_PASSWORD_JWT_SECRET_FILE
        value: /etc/secrets/reset-password-jwt-secret
      - name: AUTHELIA_SESSION_REDIS_PASSWORD_FILE
        value: /etc/redis-secrets/password
      - name: AUTHELIA_STORAGE_POSTGRES_PASSWORD_FILE
        value: /etc/postgres-secrets/user-password
    extraVolumeMounts:
      - name: authelia-secrets
        mountPath: /etc/secrets
        readOnly: true
      - name: authelia-redis-credentials
        mountPath: /etc/redis-secrets
        readOnly: true
      - name: authelia-postgresql-credentials
        mountPath: /etc/postgres-secrets
        readOnly: true
      - name: users-database
        mountPath: /config/users_database.yml
        subPath: users_database.yml
        readOnly: false
    extraVolumes:
      - name: authelia-secrets
        secret:
          secretName: authelia-secrets
      - name: authelia-redis-credentials
        secret:
          secretName: authelia-redis-credentials
      - name: authelia-postgresql-credentials
        secret:
          secretName: authelia-postgresql-credentials
      - name: users-database
        configMap:
          name: users-database
  ## Ingress configuration.
  ##
  ## Disabled to control manually
  ##
  ingress:
    enabled: false
    traefikCRD:
      enabled: false
    gatewayAPI:
      enabled: false
  ## Configuration file
  ##
  configMap:
    key: "configuration.yaml"
    theme: auto
    log:
      level: info
      format: text
    telemetry:
      metrics:
        enabled: false
    ## Enable templating in the configuration file
    ##
    filters:
      disabled: false
    server:
      endpoints:
        authz:
          ## Required for Traefik ForwardAuth middleware
          ##
          forward-auth:
            implementation: "ForwardAuth"
            authn_strategies:
              - name: "HeaderProxyAuthorization"
                schemes:
                  - "Basic"
              - name: "CookieSession"
    identity_validation:
      reset_password:
        secret:
          disabled: true
    authentication_backend:
      password_reset:
        disable: false
      password_change:
        disable: false
      refresh_interval: "5 minutes"
      file:
        enabled: true
        path: "/config/users_database.yml"
        watch: false
        search:
          email: true
          case_insensitive: false
        password:
          algorithm: argon2
          argon2:
            variant: argon2id
            iterations: 3
            memory: 65536
            parallelism: 4
            key_length: 32
            salt_length: 16
    access_control:
      default_policy: deny
      rules:
        # Admin access to Authelia admin panel
        - domain:
            - "authelia.{{ .Values.ingress.domain }}"
          policy: one_factor
          subject:
            - "group:admins"
            # Require group 'users' for all services
        - domain:
            - "{{ .Values.ingress.domain }}"
            - '{{ printf "*.%s" .Values.ingress.domain }}'
          policy: one_factor
          subject:
            - "group:users"
    session:
      ## The name of the session cookie.
      ##
      name: authelia_session
      same_site: lax
      expiration: "1 hour"
      inactivity: "5 minutes"
      remember_me: "1 month"
      encryption_key:
        # disable secret generation, we'll provide our own
        disabled: true
      cookies:
        - name: authelia_session
          domain: "{{ .Values.ingress.domain }}"
      redis:
        enabled: true
        ## Deploy the Redis bitnami chart
        ##
        deploy: true
        host: 'authelia-redis-headless.{{ include "homeserver.authelia.names.namespace" . }}.svc.cluster.local'
        port: 6379
        database_index: 0
        password:
          # disable secret generation, we'll provide our own
          disabled: true
    storage:
      encryption_key:
        # disable secret generation, we'll provide our own
        disabled: true
      postgres:
        enabled: true
        ## Deploy the PostgreSQL bitnami chart
        ##
        deploy: true
        address: 'tcp://authelia-postgresql.{{ include "homeserver.authelia.names.namespace" . }}.svc.cluster.local:5432'
        database: authelia
        schema: public
        username: authelia
        password:
          # disable secret generation, we'll provide our own
          disabled: true
    notifier:
      disable_startup_check: false
      filesystem:
        enabled: true
        filename: /config/notification.txt
      smtp:
        enabled: false
  ## Persistence configuration
  ##
  persistence: {}
  ## Service configuration
  ##
  service:
    annotations: {}
    labels: {}
  ## PostgreSQL subchart configuration
  ## See https://artifacthub.io/packages/helm/bitnami/postgresql
  ##
  postgresql:
    enabled: true
    auth:
      username: authelia
      database: authelia
      existingSecret: authelia-postgresql-credentials
      secretKeys:
        adminPasswordKey: admin-password
        userPasswordKey: user-password
      usePasswordFiles: true
    backup:
      enabled: true
      cronjob:
        schedule: "0 5 * * *"
        concurrencyPolicy: Replace
        storage:
          enabled: false
          mountPath: /backup/pgdump
        extraVolumes:
          - name: backup-dir
            hostPath:
              path: /opt/authelia/postgresql/backup
              type: Directory
        extraVolumeMounts:
          - name: backup-dir
            mountPath: /backup/pgdump
  ## Redis subchart configuration
  ## See https://artifacthub.io/packages/helm/bitnami/redis
  ##
  redis:
    enabled: true
    auth:
      enabled: true
      existingSecret: authelia-redis-credentials
      existingSecretPasswordKey: password
      usePasswordFiles: true
    master:
      resources: {}
    replica:
      replicaCount: 3
      resources: {}
## Ingress configuration
##
ingress:
  ## Your domain without a scheme
  ## e.g. 'example.com'
  ##
  domain: "" # REQUIRED
  ## The service to serve on the root domain
  ## Set it to `services.<service>.name`. Leave empty to disable.
  ##
  rootService: homepage
  ## Extra annotations to add to the Ingress resource
  ##
  extraAnnotations: {}
## VPN configuration
## To enable VPN for a service, set `services.<service_name>.vpn.enabled=true`
##
vpn:
  ## Enables SYS_MODULE capability
  ## Disable only if you know what you're doing
  ##
  sysModule: true
  ## Name of the secret containing the WireGuard configuration
  ##
  secretRef: wireguard-conf-linuxserver
  ## Key inside the secret
  ##
  secretKey: wg0.conf
## Traefik additional configuration
##
x-traefik:
  forwardedHeaders:
    ## The list of Cloudflare's IPs, which will be trusted when accepting 'X-Forwarded-*' headers
    ## See the full list at https://www.cloudflare.com/ips/
    ##
    trustedIPs:
      - 173.245.48.0/20
      - 103.21.244.0/22
      - 103.22.200.0/22
      - 103.31.4.0/22
      - 141.101.64.0/18
      - 108.162.192.0/18
      - 190.93.240.0/20
      - 188.114.96.0/20
      - 197.234.240.0/22
      - 198.41.128.0/17
      - 162.158.0.0/15
      - 104.16.0.0/13
      - 104.24.0.0/14
      - 172.64.0.0/13
      - 131.0.72.0/22
      # Needed to trust Traefik's own IPs
      - 10.0.0.0/8
## Whether information about services should be injected into pod's environment variables
## The environment variables injected by service links are usually not needed, but can lead to slow boot times.
## Recommended to keep it disabled.
## Can be overriden per service
##
enableServiceLinks: false
## Whether to mount Service Account token in pods
## Recommended to keep it disabled.
## Can be overriden per service
##
automountServiceAccountToken: false
## Affinity configuration for pods
## See https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
## Can be overriden per service
##
affinity:
  nodeAffinity: {}
  podAffinity: {}
  podAntiAffinity: {}
## Tolerations configuration for pods
## See https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
## Can be overriden per service
##
tolerations: []
## Common labels to add to all the deployed resources. Evaluated as a template
##
## Usage:
## commonLabels:
##    app.kubernetes.io/managed-by: Helm
##
commonLabels: {}
## Common annotations to add to all the deployed resources. Evaluated as a template
##
## Usage:
## commonAnnotations:
##    app.kubernetes.io/managed-by: Helm
##
commonAnnotations: {}
## Pods' global liveness probe. Evaluated as a template.
## see https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
##
livenessProbe:
  enabled: true
  initialDelaySeconds: 30
  periodSeconds: 20
  timeoutSeconds: 10
  failureThreshold: 6
  successThreshold: 1
## Pods' global readiness probe. Evaluated as a template.
## see https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes
##
readinessProbe:
  enabled: true
  initialDelaySeconds: 5
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 6
  successThreshold: 1
## Pods' global startup probe. Evaluated as a template.
## Slow starting containers can be protected through startup probes
## see https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-startup-probes
##
startupProbe:
  enabled: false
  initialDelaySeconds: 5
  periodSeconds: 20
  timeoutSeconds: 10
  failureThreshold: 30
  successThreshold: 1
## Resources configuration
##
resources:
  ## Whether resources management should be enabled
  ## see https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  enabled: false
## Fix volume permissions automatically
## Enable this if you're having permission issues
## If that solves your problem, you should properly chown/chmod your persistence directory on the host
## NOTE: it's not recommended to keep it enabled, since the init container works as a root user
##
volumePermissions:
  enabled: false
## Housekeeping configuration
##
housekeeping:
  ## Media cleanup job configuration
  ## This job will remove empty directories that possibly only contain metadata files
  ##
  mediaCleanup:
    enabled: true
    # How often to run the cleanup job (cron)
    # Every 10 min by default
    scheduleCron: "*/10 * * * *"
    # Whether to run in dry-run mode (only report which folders would be deleted)
    dryRun: false
    # Logging level (debug, info, warning, error, critical)
    logLevel: "info"
    # Persistence volumes for media directories
    #
    # Example:
    # persistence:
    #   volumes:
    #     - name: movies
    #       hostPath:
    #         path: /data/library/movies
    #         type: Directory
    #     - name: tv
    #       hostPath:
    #         path: /data/library/tv
    #         type: Directory
    #   mounts:
    #     movies:
    #       volume: movies
    #     tv:
    #       volume: tv
    persistence:
      volumes: []
      mounts: {}
    # List of mounts to scan for cleanup
    #
    # Example:
    # - name: movies                # Required: references persistence.mounts.movies
    #   maxDepth: 1                 # Optional: maximum directory depth level to scan (default: 1)
    #
    # Explanation of maxDepth:
    # - maxDepth = 0: Scans only the root directory itself.
    #                 Good for uncategorized media: root dir -> media files without subdirs.
    #                 For instance, mount /media/videos => /media/videos/*.mp4
    # - maxDepth = 1: Scans immediate subdirectories.
    #                 Good for Movies and TV shows: root dir -> movie dir.
    #                 For instance, mount /media/movies => /media/movies/Avatar/*.mkv
    # - maxDepth = 2: Scans one level deeper.
    #                 Good for Music: root dir -> author dir -> album dir.
    #                 For instance, mount /media/music => /media/music/Eminem/Relapse/*.m4a
    #
    paths: []
    # List of file globs considered as metadata files
    # See https://docs.python.org/3/library/fnmatch.html for more information about the glob patterns
    metadataFileGlobs:
      - "*.nfo"
      - "*.json"
      - "*.jpg"
      - "*.jpeg"
      - "*.png"
      - "*.svg"
      - "*.sub"
      - "*.srt"
      - "*.vtt"
      - "*.idx"
      - "*.trickplay"
      - "theme.mp3"
      - ".DS_Store"
      - "Thumbs.db"
## Monitoring configuration
## This will install and configure Grafana and Loki
##
monitoring:
  ## Whether to enable monitoring
  enabled: true
  ## Namespace to deploy monitoring resources to
  namespace: monitoring
  ## Loki chart configuration
  ## See available values https://github.com/grafana/loki/blob/main/production/helm/loki/values.yaml
  ##
  loki:
    global:
      dnsService: "kube-dns"
      dnsNamespace: "kube-system"
      extraArgs:
        - "-log.level=info"
        - "-config.expand-env=true"
        - "-log-config-reverse-order"
      extraEnv:
        - name: S3_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: loki-s3-credentials
              key: S3_ACCESS_KEY
        - name: S3_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: loki-s3-credentials
              key: S3_SECRET_KEY
    loki:
      auth_enabled: false
      ## https://grafana.com/docs/loki/latest/configure/#common
      commonConfig:
        replication_factor: 1
      analytics:
        reporting_enabled: false
      ## Storage config to be shared between multiple modules
      ## https://grafana.com/docs/loki/latest/configure/#common
      storage:
        type: s3
        bucketNames:
          chunks: homeserver-loki-chunks
          ruler: homeserver-loki-ruler
          admin: homeserver-loki-admin
        ## S3 Storage Config
        ## https://grafana.com/docs/loki/latest/configure/#s3_storage_config
        s3:
          endpoint: https://storage.yandexcloud.net
          accessKeyId: ${S3_ACCESS_KEY}
          secretAccessKey: ${S3_SECRET_KEY}
          region: ru-central1
          s3ForcePathStyle: false
          insecure: false
          # Connection timeouts and limits
          tls_handshake_timeout: 10s
          expect_continue_timeout: 1s
          max_idle_connections: 50
          max_idle_connections_per_host: 25
          max_connections_per_host: 50
      ## Storage configuration for object storage
      ## https://grafana.com/docs/loki/latest/configure/#storage_config
      storage_config:
        tsdb_shipper:
          active_index_directory: /var/loki/index
          cache_location: /var/loki/index_cache
          # Can be increased for faster performance over longer query periods, uses more disk space
          cache_ttl: 24h
      # Query range configuration for parallelization and caching
      query_range:
        # Enable parallelization of shardable queries to improve performance
        parallelise_shardable_queries: true
        # Align queries with step intervals for better caching
        align_queries_with_step: true
        # Enable caching of query results
        cache_results: true
        # Maximum number of retries for failed queries
        max_retries: 3
      schemaConfig:
        configs:
          - from: 2025-05-01
            store: tsdb
            object_store: s3
            schema: v13
            index:
              prefix: loki_index_
              period: 24h
      pattern_ingester:
        enabled: true
      ## Ingester config
      ## https://grafana.com/docs/loki/latest/configure/#ingester
      ##
      ingester:
        # algorithm to use for compressing chunk
        # snappy is a best practice choice for most use cases
        # read https://grafana.com/docs/loki/latest/configure/bp-configure/#use-snappy-compression-algorithm
        chunk_encoding: snappy
        # target _compressed_ size in bytes for chunks
        # read https://grafana.com/docs/loki/latest/configure/bp-configure/#use-chunk_target_size
        chunk_target_size: 1572864 # 1.5MB
        # the maximum duration of a timeseries chunk in memory, regardless of activity
        max_chunk_age: 2h
        # flush on inactivity
        # if you don't have much log volume, consider increasing this value to reduce tiny chunks
        chunk_idle_period: 1h
        # how long a closed chunk is kept in the ingester’s memory after it’s been flushed
        chunk_retain_period: 5m
        # WAL configuration
        wal:
          enabled: true
          replay_memory_ceiling: 2147483648 # 2GB in bytes
        lifecycler:
          # Address to advertise in consul
          address: 0.0.0.0
          ring:
            heartbeat_timeout: 1m
            replication_factor: 1
      tracing:
        enabled: true
      ## https://grafana.com/docs/loki/latest/configure/#limits_config
      limits_config:
        # Allow user to send structured metadata in push payload
        allow_structured_metadata: true
        # Discover and add log levels during ingestion, if not present already
        discover_log_levels: true
        volume_enabled: true
        ## Retention period
        ##
        retention_period: 2160h # 90 days
        # Lookback limit to prevent querying too far back
        #
        # To avoid querying of data beyond the retention period,
        # max_query_lookback must be set to a value less than or equal retention_period
        max_query_lookback: 2160h # 90 days
        # Rate limiting to prevent storage spam
        ingestion_rate_mb: 32 # Max 32MB per second ingestion rate per user
        ingestion_burst_size_mb: 64 # Allow bursts up to 64MB
        max_streams_per_user: 10000 # Limit concurrent streams per user
        max_line_size: 262144 # Limit individual log line size (256KB in bytes)
        max_entries_limit_per_query: 50000 # Limit query result size
        # Reject old samples to prevent replay storms
        reject_old_samples: true
        reject_old_samples_max_age: 24h # Reject samples older than 1 day
        # Query timeout limits
        query_timeout: 300s # 5 minute query timeout for individual queries
        max_query_parallelism: 16 # Limit concurrent query workers
        # Chunk limits to prevent memory issues during storage failures
        max_chunks_per_query: 2000000 # 2M chunks max per query
        max_query_series: 10000 # Limit series per query
        # Split queries by interval to reduce load
        split_queries_by_interval: 15m
        # Cardinality limits to prevent explosion during failures
        cardinality_limit: 100000
        max_streams_matchers_per_query: 1000
      ## https://grafana.com/docs/loki/latest/configure/#ruler
      ruler:
        enable: true
      ## https://grafana.com/docs/loki/latest/configure/#querier
      querier:
        # Default is 4, if you have enough memory and CPU you can increase, reduce if OOMing
        max_concurrent: 4
        # Engine configuration
        engine:
          max_look_back_period: 30s # Limit lookback to reduce storage load
      ## Compactor configuration
      ## https://grafana.com/docs/loki/latest/configure/#compactor
      ##
      compactor:
        working_directory: /var/loki/compactor
        compaction_interval: 2h
        retention_enabled: true
        retention_delete_delay: 2h
        retention_delete_worker_count: 10
        # A separate cadence for applying retention only.
        # By default it’s 0s, meaning retention is applied on every compaction_interval.
        # Use this when you want compaction to run frequently but retention scans less often
        apply_retention_interval: 12h
        # Required: delete request store must be configured when retention is enabled
        delete_request_store: s3
        retention_backoff_config:
          min_period: 500ms
          max_period: 300s
          max_retries: 5
      frontend:
        # Response compression
        compress_responses: true
        # Max outstanding requests per tenant
        max_outstanding_per_tenant: 100
      server:
        # Timeout configurations to handle storage failures gracefully
        http_server_read_timeout: 300s
        http_server_write_timeout: 300s
        http_server_idle_timeout: 120s
        grpc_server_max_recv_msg_size: 4194304 # 4MB
        grpc_server_max_send_msg_size: 4194304 # 4MB
        graceful_shutdown_timeout: 30s
    deploymentMode: SingleBinary
    singleBinary:
      replicas: 1
      # Enhanced resource configuration to prevent OOM during storage failures
      resources:
        limits:
          cpu: 2
          memory: 2Gi
        requests:
          cpu: 500m
          memory: 256m
      extraArgs:
        - "-log.level=info"
        - "-config.expand-env=true"
        - "-log-config-reverse-order"
      extraEnv:
        - name: S3_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: loki-s3-credentials
              key: S3_ACCESS_KEY
        - name: S3_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: loki-s3-credentials
              key: S3_SECRET_KEY
    chunksCache:
      enabled: true
      # Amount of memory allocated to chunks-cache for object storage (in MB)
      # Increase the limit if you have sufficient memory, this should make queries faster
      allocatedMemory: 64
      # default is 500MB, with limited memory keep this smaller
      writebackSizeLimit: 10MB
    resultsCache:
      # Enable results cache if you have sufficient memory
      enabled: true
      # Amount of memory allocated to results-cache for object storage (in MB)
      # Increase the limit if you have sufficient memory, this should make queries faster
      allocatedMemory: 32
      # default is 500MB, with limited memory keep this smaller
      writebackSizeLimit: 10MB
    # Zero out replica counts of other deployment modes
    backend:
      replicas: 0
    read:
      replicas: 0
    write:
      replicas: 0
    ingester:
      replicas: 0
    querier:
      replicas: 0
    queryFrontend:
      replicas: 0
    queryScheduler:
      replicas: 0
    distributor:
      replicas: 0
    compactor:
      replicas: 0
    indexGateway:
      replicas: 0
    bloomCompactor:
      replicas: 0
    bloomGateway:
      replicas: 0
  ## Grafana chart configuration
  ## See available values https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
  ##
  grafana:
    grafana.ini:
      analytics:
        reporting_enabled: false
        feedback_links_enabled: false
        check_for_updates: false
        check_for_plugin_updates: false
      server:
        root_url: '{{ printf "https://grafana.%s" .Values.ingress.domain }}'
      auth:
        disable_login_form: false
        disable_signout_menu: false
      ## Auth via Authentik
      ## See https://grafana.com/docs/grafana/latest/setup-grafana/configure-security/configure-authentication/generic-oauth/
      ## Make sure to pass client ID and client secret via env vars below
      ##
      auth.generic_oauth:
        enabled: false
        name: Authentik
        scopes: "openid profile email groups"
        auth_url: https://auth.example.com/application/o/authorize/
        token_url: https://auth.example.com/application/o/token/
        api_url: https://auth.example.com/application/o/userinfo/
        login_attribute_path: preferred_username
        email_attribute_path: email
        role_attribute_path: contains(groups[*], 'grafana_admin') && 'Admin' || 'Viewer'
    envValueFrom: {}
    # GF_AUTH_GENERIC_OAUTH_CLIENT_ID:
    #   secretKeyRef:
    #     name: grafana-authentik-oauth-client-secret
    #     key: client-id
    # GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET:
    #   secretKeyRef:
    #     name: grafana-authentik-oauth-client-secret
    #     key: client-secret
    persistence:
      type: pvc
      enabled: true
    initChownData:
      enabled: true
    ## Grafana admin credentials
    admin:
      # Name of the existing secret
      existingSecret: grafana-admin-credentials
      userKey: username
      passwordKey: password
    service:
      enabled: true
      type: ClusterIP
    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
          - name: Loki
            type: loki
            access: proxy
            orgId: 1
            url: "http://loki-gateway:80"
            basicAuth: false
            isDefault: true
            editable: false
            version: 1
  ## k8s monitoring chart configuration
  ## See overview of this chart - https://grafana.com/docs/grafana-cloud/monitor-infrastructure/kubernetes-monitoring/configuration/helm-chart-config/helm-chart/
  ##
  ## Docs:
  ## - Structure: https://github.com/grafana/k8s-monitoring-helm/blob/main/charts/k8s-monitoring/docs/Structure.md
  ## - Features: https://github.com/grafana/k8s-monitoring-helm/blob/main/charts/k8s-monitoring/docs/Features.md
  ## - Collectors: https://github.com/grafana/k8s-monitoring-helm/blob/main/charts/k8s-monitoring/docs/Collectors.md
  ##
  ## See available values https://github.com/grafana/k8s-monitoring-helm/blob/main/charts/k8s-monitoring/values.yaml
  ##
  k8s-monitoring:
    cluster:
      ## This a static label that will be attached to all collected logs
      name: homeserver
    destinations:
      - name: loki
        type: loki
        url: '{{ printf "http://loki-gateway.%s.svc.cluster.local/loki/api/v1/push" (include "homeserver.monitoring.names.namespace" .) }}'
    # --------
    # Features
    # --------

    # Gathers Kubernetes lifecycle events as log data
    clusterEvents:
      enabled: true
      collector: alloy-singleton
      labelsToKeep:
        - "level"
        - "namespace"
        - "node"
        - "source"
        - "reason"
        - "job"
      structuredMetadata:
        name: name
      # filter namespaces
      # you can add your other namespaces to scrape
      # [] means all namespaces
      namespaces:
        - '{{ include "homeserver.common.names.namespace" . }}'
        - '{{ include "homeserver.authentik.names.namespace" . }}'
        - '{{ include "homeserver.monitoring.names.namespace" . }}'
        - kube-system
    # Gathers logs from the Kubernetes Nodes
    nodeLogs:
      enabled: true
      gatherMethod: volumes
      collector: alloy-logs
      journal:
        maxAge: "8h"
        path: "/var/log/journal"
        # The list of systemd units to keep scraped logs from, this can be a valid RE2 regex.
        # If empty, all units are scraped.
        units: []
    # Gathers logs from the Kubernetes Pods
    #
    # You can obtain the final Alloy config by going to the respective collector's pod
    # and executing `cat /etc/alloy/config.alloy`
    podLogs:
      enabled: true
      gatherMethod: volumes
      collector: alloy-logs
      # remap labels
      # target: source
      labels:
        app_kubernetes_io_name: app.kubernetes.io/name
        app_kubernetes_io_component: app.kubernetes.io/component
        app: app_kubernetes_io_name
        component: app_kubernetes_io_component
        homeserver_vpn: homeserver/vpn
      # extra discovery.relabel rules
      # see https://grafana.com/docs/alloy/latest/reference/components/discovery/discovery.relabel/
      extraDiscoveryRules: |-
        // add more labels
        rule {
          action        = "replace"
          source_labels = ["__meta_kubernetes_pod_node_name"]
          target_label  = "node_name"
        }
        rule {
          action        = "replace"
          source_labels = ["__meta_kubernetes_pod_container_image"]
          target_label  = "container_image"
        }
        rule {
          action        = "replace"
          source_labels = ["__meta_kubernetes_pod_container_init"]
          target_label  = "container_init"
        }
      # extra loki.process rules
      # see https://grafana.com/docs/alloy/latest/reference/components/loki/loki.process/
      extraLogProcessingStages:
      # indexed labels
      labelsToKeep:
        - service_name
        - app
        - component
        - container
        - container_image
        - container_init
        - node_name
        - namespace
        - level
        - homeserver_vpn
      # unindexed labels
      # high-cardinality labels should go here
      structuredMetadata:
        pod: pod
      # filter namespaces
      # you can add your other namespaces to scrape
      # [] means all namespaces
      namespaces:
        - '{{ include "homeserver.common.names.namespace" . }}'
        - '{{ include "homeserver.authentik.names.namespace" . }}'
        - '{{ include "homeserver.monitoring.names.namespace" . }}'
        - kube-system
    # Gathers metrics related the the Kubernetes Cluster itself
    clusterMetrics:
      enabled: false
      collector: alloy-metrics
      # filter namespaces
      # you can add your other namespaces to scrape
      # [] means all namespaces
      namespaces:
        - '{{ include "homeserver.common.names.namespace" . }}'
        - '{{ include "homeserver.authentik.names.namespace" . }}'
        - '{{ include "homeserver.monitoring.names.namespace" . }}'
        - kube-system
    # ----------
    # Collectors
    # ----------

    # The Grafana Alloy instance that is responsible for anything that must be done on a single instance,
    # such as gathering Cluster events from the API server.
    # It is deployed as a ReplicaSet with one replica
    alloy-singleton:
      enabled: true
      alloy:
        resources:
          requests: {}
          limits: {}
        mounts:
          dockercontainers: false
          varlog: false
    # The Grafana Alloy instance that scrapes workload logs on each node.
    # It is deployed as a DaemonSet with one replica per node
    alloy-logs:
      enabled: true
      alloy:
        resources:
          requests: {}
          limits: {}
        mounts:
          dockercontainers: false
          # required for node & pod logs
          varlog: true
        clustering:
          enabled: false
    # The Grafana Alloy instance that is responsible for scraping metrics from prometheus sources like cadvisor and kube-state-metrics
    # It is deployed as a StatefulSet
    alloy-metrics:
      enabled: false
      alloy:
        resources:
          requests: {}
          limits: {}
        clustering:
          enabled: false
    # The Grafana Alloy instance that opens receiver ports to process data delivered directly to Alloy (HTTP, gRPC, Zipkin, etc)
    # For example, applications instrumented with OpenTelemetry SDKs
    # It is deployed as a Daemonset
    alloy-receiver:
      enabled: false
    alloy-profiles:
      enabled: false
services:
  backrest:
    enabled: false
    exposed: true
    name: backrest
    replicaCount: 1
    critical: false
    image:
      repository: garethgeorge/backrest
      tag: v1.10
      pullPolicy: IfNotPresent
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 9898
    ingress:
      - backrest
      - backup
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: userdata
    #   hostPath:
    #     path: /opt
    #     type: Directory
    # - name: config
    #   hostPath:
    #     path: /opt/backrest/config
    #     type: DirectoryOrCreate
    # - name: data
    #   hostPath:
    #     path: /opt/backrest/data
    #     type: DirectoryOrCreate
    # - name: cache
    #   hostPath:
    #     path: /opt/backrest/cache
    #     type: DirectoryOrCreate
    # mounts:
    #   userdata:
    #     volume: userdata
    #   config:
    #     volume: config
    #   data:
    #     volume: data
    #   cache:
    #     volume: cache
    persistence:
      volumes: []
      mounts:
        userdata:
          volume: ""
        config:
          volume: ""
        data:
          volume: ""
        cache:
          volume: ""
    vpn: {}
    extraEnv: []
    extraEnvSecrets:
      - name: RESTIC_PASSWORD
        secretName: backrest-restic-password-secret
        secretKey: restic-password
      - name: AWS_ACCESS_KEY_ID
        secretName: backrest-aws-s3-yandex-cloud-secret
        secretKey: aws-access-key-id
      - name: AWS_SECRET_ACCESS_KEY
        secretName: backrest-aws-s3-yandex-cloud-secret
        secretKey: aws-secret-access-key
  homepage:
    enabled: true
    exposed: true
    name: homepage
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/gethomepage/homepage
      tag: v1.6.0
      pullPolicy: IfNotPresent
    securityContext:
      strict: false
    resourcesPreset: "micro"
    ports:
      http: 3000
    ingress:
      - homepage
    vpn: {}
    extraEnvSecrets:
      - name: HOMEPAGE_VAR_RADARR_API_KEY
        secretName: homepage-widget-radarr-secret
        secretKey: api-key
      - name: HOMEPAGE_VAR_SONARR_API_KEY
        secretName: homepage-widget-sonarr-secret
        secretKey: api-key
      - name: HOMEPAGE_VAR_LIDARR_API_KEY
        secretName: homepage-widget-lidarr-secret
        secretKey: api-key
      - name: HOMEPAGE_VAR_BAZARR_API_KEY
        secretName: homepage-widget-bazarr-secret
        secretKey: api-key
      - name: HOMEPAGE_VAR_AUTOBRR_API_KEY
        secretName: homepage-widget-autobrr-secret
        secretKey: api-key
      - name: HOMEPAGE_VAR_PROWLARR_API_KEY
        secretName: homepage-widget-prowlarr-secret
        secretKey: api-key
      - name: HOMEPAGE_VAR_SABNZBD_API_KEY
        secretName: homepage-widget-sabnzbd-secret
        secretKey: api-key
      - name: HOMEPAGE_VAR_JELLYSEERR_API_KEY
        secretName: homepage-widget-jellyseerr-secret
        secretKey: api-key
      - name: HOMEPAGE_VAR_TAUTULLI_API_KEY
        secretName: homepage-widget-tautulli-secret
        secretKey: api-key
      - name: HOMEPAGE_VAR_JELLYFIN_API_KEY_0
        secretName: homepage-widget-jellyfin-secret
        secretKey: api-key
      - name: HOMEPAGE_VAR_JELLYFIN_API_KEY_1
        secretName: homepage-widget-jellyfin-secret
        secretKey: api-key
      - name: HOMEPAGE_VAR_CALIBRE_USERNAME
        secretName: homepage-widget-calibre-secret
        secretKey: username
      - name: HOMEPAGE_VAR_CALIBRE_PASSWORD
        secretName: homepage-widget-calibre-secret
        secretKey: password
      - name: HOMEPAGE_VAR_KAVITA_USERNAME
        secretName: homepage-widget-kavita-secret
        secretKey: username
      - name: HOMEPAGE_VAR_KAVITA_PASSWORD
        secretName: homepage-widget-kavita-secret
        secretKey: password
      - name: HOMEPAGE_VAR_MINIFLUX_API_KEY
        secretName: homepage-widget-miniflux-secret
        secretKey: api-key
      - name: HOMEPAGE_VAR_CHANGEDETECTION_API_KEY
        secretName: homepage-widget-changedetection-secret
        secretKey: api-key
      - name: HOMEPAGE_VAR_GOTIFY_API_KEY
        secretName: homepage-widget-gotify-secret
        secretKey: api-key
      - name: HOMEPAGE_VAR_MEALIE_API_KEY
        secretName: homepage-widget-mealie-secret
        secretKey: api-key
      - name: HOMEPAGE_VAR_GRAFANA_USERNAME
        secretName: homepage-widget-grafana-secret
        secretKey: username
      - name: HOMEPAGE_VAR_GRAFANA_PASSWORD
        secretName: homepage-widget-grafana-secret
        secretKey: password
      - name: HOMEPAGE_VAR_AUTHENTIK_API_KEY
        secretName: homepage-widget-authentik-secret
        secretKey: api-key
    extraEnv: []
  plex:
    enabled: false
    exposed: true
    name: plex
    replicaCount: 1
    critical: true
    image:
      repository: ghcr.io/home-operations/plex
      tag: rolling
      pullPolicy: Always
    securityContext:
      strict: true
    resourcesPreset: "large"
    ports:
      http: 8080
      # make sure to open the remote access port in your firewall
      # you can change this port but it should be in range of 30000-32767 (k8s NodePort)
      remoteAccess: 32400
    ingress:
      - plex
    claim: ""
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/plex/config
    #     type: DirectoryOrCreate
    # - name: library
    #   hostPath:
    #     path: /data/library
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    #   library:
    #     volume: library
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
        library:
          volume: ""
    extraEnv: []
  jellyfin:
    - enabled: false
      exposed: true
      name: jellyfin
      replicaCount: 1
      critical: false
      image:
        repository: ghcr.io/jellyfin/jellyfin
        tag: "10"
        pullPolicy: Always
      securityContext:
        strict: true
      resourcesPreset: "large"
      ports:
        http: 8096
        udp: 7359
      ingress:
        - jellyfin
      vpn: {}
      persistence:
        volumes:
          - name: config
            hostPath:
              path: /opt/jellyfin/config
              type: DirectoryOrCreate
          - name: cache
            hostPath:
              path: /opt/jellyfin/cache
              type: DirectoryOrCreate
          - name: library
            hostPath:
              path: /data/library
              type: DirectoryOrCreate
        mounts:
          config:
            volume: config
          cache:
            volume: cache
          library:
            volume: library
      extraEnv: []
  autobrr:
    enabled: false
    exposed: true
    name: autobrr
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/autobrr/autobrr
      tag: v1.68
      pullPolicy: Always
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 7474
    ingress:
      - autobrr
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/autobrr/config
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
    vpn: {}
    # see the full list of supported env vars
    # https://autobrr.com/installation/docker#environment-variables
    #
    extraEnv: []
  qbittorrent:
    enabled: false
    exposed: true
    name: qbittorrent
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/home-operations/qbittorrent
      tag: "5.1.3"
      pullPolicy: IfNotPresent
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 8080
      # make sure the p2p port is allowed in your firewall
      # you can change this port but it should be in range of 30000-32767 (k8s NodePort)
      p2p: 32700
    ingress:
      - qbittorrent
      - torrent
    ## VueTorrent configuration
    ## see https://github.com/VueTorrent/VueTorrent
    ##
    ## After enabling, set '/vuetorrent' in the qBittorrent -> Web UI settings
    vuetorrent:
      enabled: true
      # must match the release version https://github.com/VueTorrent/VueTorrent/releases
      version: v2.24.2
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/qbittorrent/config
    #     type: DirectoryOrCreate
    # - name: data
    #   hostPath:
    #     path: /data/torrents
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    #   data:
    #     volume: data
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
        data:
          volume: ""
    vpn: {}
    extraEnv: []
  qbit_manage:
    enabled: false
    exposed: false
    name: qbit-manage
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/stuffanthings/qbit_manage
      tag: v4.6.4
      pullPolicy: IfNotPresent
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports: {}
    ingress: []
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/qbit_manage/config
    #     type: DirectoryOrCreate
    # - name: data
    #   hostPath:
    #     path: /data/torrents
    #     type: Directory
    # mounts:
    #   config:
    #     volume: config
    #   # This MUST point to the same volume (and optionally subPath) as qbittorrent data mount
    #   qbittorrent-data:
    #     volume: data
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
        qbittorrent-data:
          volume: ""
    scheduleCron: "*/10 * * * *"
    vpn: {}
    extraEnv:
      - name: QBT_LOG_LEVEL
        value: INFO
  sabnzbd:
    enabled: false
    exposed: true
    name: sabnzbd
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/home-operations/sabnzbd
      tag: "4"
      pullPolicy: Always
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 8080
    ingress:
      - sabnzbd
      - usenet
    vpn: {}
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/sabnzbd/config
    #     type: DirectoryOrCreate
    # - name: data
    #   hostPath:
    #     path: /data/usenet
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    #   data:
    #     volume: data
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
        data:
          volume: ""
    extraEnv: []
  prowlarr:
    enabled: false
    exposed: true
    name: prowlarr
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/home-operations/prowlarr
      tag: "2"
      pullPolicy: Always
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 9696
    ingress:
      - prowlarr
    vpn: {}
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/prowlarr/config
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
    extraEnv: []
  radarr:
    enabled: false
    exposed: true
    name: radarr
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/home-operations/radarr
      tag: "6"
      pullPolicy: Always
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 7878
    ingress:
      - radarr
      - movies
      - movie
    vpn: {}
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/radarr/config
    #     type: DirectoryOrCreate
    # - name: data
    #   hostPath:
    #     path: /data
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    #   data:
    #     volume: data
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
        data:
          volume: ""
    extraEnv: []
  sonarr:
    enabled: false
    exposed: true
    name: sonarr
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/home-operations/sonarr
      tag: "4"
      pullPolicy: Always
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 8989
    ingress:
      - sonarr
      - tv
    vpn: {}
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/sonarr/config
    #     type: DirectoryOrCreate
    # - name: data
    #   hostPath:
    #     path: /data
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    #   data:
    #     volume: data
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
        data:
          volume: ""
    extraEnv: []
  lidarr:
    enabled: false
    exposed: true
    name: lidarr
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/home-operations/lidarr
      tag: "2.14"
      pullPolicy: Always
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 8686
    ingress:
      - lidarr
      - music
    vpn: {}
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/lidarr/config
    #     type: DirectoryOrCreate
    # - name: data
    #   hostPath:
    #     path: /data
    #     type: DirectoryOrCreate
    # - name: downloads
    #   hostPath:
    #     path: /data/soulseek/downloaded
    #     type: Directory
    # mounts:
    #   config:
    #     volume: config
    #   data:
    #     volume: data
    #   soulseek-downloads:
    #     volume: downloads
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
        data:
          volume: ""
        ## Optional - only for Soulseek integration
        ## This MUST point to the same volume (and optionally subPath) as slskd downloads mount
        soulseek-downloads:
          volume: ""
    extraEnv: []
  slskd:
    enabled: false
    exposed: true
    name: slskd
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/slskd/slskd
      tag: 0.24.0
      pullPolicy: Always
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 5030
      incoming: 50300
    ingress:
      - slskd
      - soulseek
    vpn: {}
    # Persistence config
    #
    # Example:
    # volumes:
    #   - name: config
    #     hostPath:
    #       path: /opt/slskd/config
    #       type: DirectoryOrCreate
    #   - name: downloads
    #     hostPath:
    #       path: /data/soulseek/downloaded
    #       type: DirectoryOrCreate
    #   - name: incomplete
    #     hostPath:
    #       path: /data/soulseek/incomplete
    #       type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    #   downloads:
    #     volume: downloads
    #   incomplete:
    #     volume: incomplete
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
        downloads:
          volume: ""
        incomplete:
          volume: ""
    config:
      ## Configure the container directories you want to share.
      ## Make sure to inject the corresponding volume via `extraVolumes` and `extraVolumeMounts`.
      ##
      ## Example:
      ## shares:
      ## - containerPath: "/shares/music"
      ## - containerPath: "/shares/movies"
      ## - containerPath: "/shares/books"
      shares: []
      ## API keys for service integrations
      ## Each entry creates an API key with the specified name
      ##
      ## Example:
      ## apiKeys:
      ## - name: "soularr"
      ##   secretRef:
      ##     name: "soularr-api-key"
      ##     key: "api-key"
      ##   cidr: "10.0.0.0/8"
      ## - name: "radarr"
      ##   secretRef:
      ##     name: "radarr-api-key"
      ##     key: "api-key"
      apiKeys: []
      ## Web UI authentication
      ##
      auth:
        enabled: true
        secret:
          name: "" # Name of the secret containing Web UI username and password
          usernameKey: "username" # Key for username within the secret
          passwordKey: "password" # Key for password within the secret
      ## Soulseek network credentials secret (optional)
      ## Create secret: kubectl create secret generic slskd-credentials --from-literal=username=your-username --from-literal=password=your-password
      credentialsSecret:
        name: "" # OPTIONAL - Secret name containing Soulseek network username and password
        usernameKey: "username" # Key for username within the secret
        passwordKey: "password" # Key for password within the secret
    extraEnv: []
  soularr:
    enabled: false
    exposed: false
    name: soularr
    replicaCount: 1
    critical: false
    image:
      repository: mrusse08/soularr
      tag: latest
      pullPolicy: Always
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports: {}
    ingress: []
    ## script runs continuously with this interval (in seconds)
    scriptInterval: 300
    vpn: {}
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/soularr/config
    #     type: DirectoryOrCreate
    # - name: downloads
    #   hostPath:
    #     path: /data/soulseek/downloaded
    #     type: Directory
    # mounts:
    #   config:
    #     volume: config
    #   # this MUST point to the same volume (and optionally subPath) as slskd downloads mount
    #   soulseek-downloads:
    #     volume: downloads
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
        soulseek-downloads:
          volume: ""
    lidarr:
      apiKeySecret:
        name: "" # Name of the secret containing the Lidarr API key
        key: "api-key" # Key within the secret containing the API key
    ## Slskd integration
    slskd:
      apiKeySecret:
        name: "" # OPTIONAL - Secret name containing API key for Soularr integration
        key: "api-key" # Key within the secret
    extraEnv: []
  whisparr:
    enabled: false
    exposed: true
    name: whisparr
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/hotio/whisparr
      tag: v3
      pullPolicy: Always
    securityContext:
      # the Hotio image uses s6-overlay,
      # which has its own mechanism for switching to a non-root user via PUID/GUID
      strict: false
    resourcesPreset: "none"
    ports:
      http: 6969
    ingress:
      - whisparr
      - porn
      - xxx
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/whisparr/config
    #     type: DirectoryOrCreate
    # - name: data
    #   hostPath:
    #     path: /data
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    #   data:
    #     volume: data
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
        data:
          volume: ""
  bazarr:
    enabled: false
    exposed: true
    name: bazarr
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/home-operations/bazarr
      tag: "1"
      pullPolicy: Always
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 6767
    ingress:
      - bazarr
      - subtitles
      - subtitle
    vpn: {}
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/bazarr/config
    #     type: DirectoryOrCreate
    # - name: data
    #   hostPath:
    #     path: /data
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    #   data:
    #     volume: data
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
        data:
          volume: ""
    extraEnv: []
  jellyseerr:
    enabled: false
    exposed: true
    name: jellyseerr
    replicaCount: 1
    critical: false
    image:
      repository: fallenbagel/jellyseerr
      tag: "2.7.3"
      pullPolicy: IfNotPresent
    securityContext:
      strict: false
    resourcesPreset: "micro"
    ports:
      http: 5055
    ingress:
      - jellyseerr
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/jellyseerr/config
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
    extraEnv: []
  flaresolverr:
    enabled: false
    exposed: false
    name: flaresolverr
    replicaCount: 1
    critical: false
    image:
      # up to date fork of FlareSolverr
      # uses nodriver method instead of obsolete undetected-chromedriver
      repository: 21hsmw/flaresolverr
      tag: nodriver
      pullPolicy: Always
    securityContext:
      # throws exception if strict, hard to debug
      strict: false
    resourcesPreset: "micro"
    ports:
      http: 8191
    ingress: []
    vpn: {}
    # nodriver is a preferred method
    # it's the official successor of undetected-chromedriver
    # do not change this value unless you know what you are doing
    driver: nodriver
    logLevel: INFO
    lang: en_US
    # see the full list of supported env vars
    # https://github.com/21hsmw/FlareSolverr?tab=readme-ov-file#environment-variables
    #
    extraEnv: []
  cloudflarebypassforscraping:
    enabled: false
    exposed: false
    name: cloudflarebypassforscraping
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/sarperavci/cloudflarebypassforscraping
      tag: latest
      pullPolicy: Always
    securityContext:
      strict: false
    resourcesPreset: "micro"
    ports:
      http: 8000
    ingress: []
    vpn: {}
    extraEnv: []
  tautulli:
    enabled: false
    exposed: true
    name: tautulli
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/home-operations/tautulli
      tag: "2"
      pullPolicy: Always
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 8181
    ingress:
      - tautulli
    vpn: {}
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/tautulli/config
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
    extraEnv: []
  kometa:
    enabled: false
    exposed: false
    name: kometa
    replicaCount: 1
    critical: false
    image:
      repository: kometateam/kometa
      tag: v2.2.2
      pullPolicy: IfNotPresent
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 8181
    ingress: []
    vpn: {}
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/kometa/config
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
    scheduleCron: "0 5 * * *"
    ## Jmxd overlays configuration
    ## See https://github.com/jmxd/Kometa
    ##
    jmxdOverlays:
      enabled: false
      commitHash: 75a1c1f83d62d4b7d334d2dffa90e6865649ee39
    connections:
      tmdb:
        apiKey: "" # REQUIRED
        language: en
        region: US
      imdb:
        userId: "" # REQUIRED
      letterboxd:
        username: "" # REQUIRED
      plex:
        apiKey: "" # REQUIRED
      radarr:
        apiKey: "" # go to Settings -> General and retrieve the API key
        rootFolderPath: /data/library/movies
        qualityProfile: "Best 1080p"
      sonarr:
        apiKey: "" # go to Settings -> General and retrieve the API key
        rootFolderPath: /data/library/tv
        qualityProfile: "Best 1080p"
    ## see the full list of supported env vars
    ## https://kometa.wiki/en/latest/kometa/environmental
    ##
    extraEnv: []
  thelounge:
    enabled: false
    exposed: true
    name: thelounge
    replicaCount: 1
    critical: false
    image:
      repository: thelounge/thelounge
      tag: "4"
      pullPolicy: Always
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 9000
    ingress:
      - thelounge
      - lounge
      - irc
    vpn: {}
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/thelounge/config
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
    extraEnv: []
  myspeed:
    enabled: false
    exposed: true
    name: myspeed
    replicaCount: 1
    critical: false
    image:
      repository: germannewsmaker/myspeed
      tag: "1.0.9"
      pullPolicy: IfNotPresent
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 5216
    ingress:
      - myspeed
      - speedtest
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/myspeed/config
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
    extraEnv: []
  stirlingpdf:
    enabled: false
    exposed: true
    name: stirlingpdf
    replicaCount: 1
    critical: false
    image:
      repository: frooodle/s-pdf
      tag: "1.4.0-ultra-lite"
      pullPolicy: IfNotPresent
    securityContext:
      # unfortunately the image needs root access
      # in order to install packages
      strict: false
    resourcesPreset: "micro"
    ports:
      http: 8080
    ingress:
      - stirlingpdf
      - stirling-pdf
      - pdf
    vpn: {}
    enableLogin: false
    installBookAndAdvancedHtmlOps: true
    langs: "en_GB,en_US,ru_RU"
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/stirlingpdf/config
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
    extraEnv: []
    ## see the full list of supported env vars
    ## https://github.com/Stirling-Tools/Stirling-PDF?tab=readme-ov-file#customisation
    ##
    # - name: UI_APP_NAME
    #   value: "My very own Stirling PDF"
  miniflux:
    enabled: false
    exposed: true
    name: miniflux
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/miniflux/miniflux
      tag: "2.2.14"
      pullPolicy: IfNotPresent
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 8080
    ingress:
      - miniflux
      - rss
    vpn: {}
    # postgresql bitnami subchart
    # https://artifacthub.io/packages/helm/bitnami/postgresql#parameters
    postgresql:
      primary:
        resources:
          limits:
            cpu: 200m
            ephemeral-storage: 1Gi
            memory: 192Mi
          requests:
            cpu: 100m
            ephemeral-storage: 50Mi
            memory: 128Mi
      backup:
        enabled: true
        cronjob:
          schedule: "0 5 * * *"
          concurrencyPolicy: Replace
          storage:
            # disable PVC, we'll save backups to a local dir
            enabled: false
            mountPath: /backup/pgdump
          extraVolumes:
            - name: backup-dir
              hostPath:
                path: /opt/miniflux/postgresql/backup
                type: Directory
          extraVolumeMounts:
            - name: backup-dir
              mountPath: /backup/pgdump
    # number of days after which marking read items as removed; set to -1 to disable this feature
    cleanupArchiveReadDays: 3600
    # number of days after which marking unread items as removed; set to -1 to disable this feature
    cleanupArchiveUnreadDays: 360
    # cleanup job frequency
    cleanupFrequencyHours: 24
    extraEnv: []
  huginn:
    enabled: false
    exposed: true
    name: huginn
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/huginn/huginn
      tag: 1e0c359a46b1e84eb8c658404212eaf693b30e61
      pullPolicy: IfNotPresent
    securityContext:
      strict: false
    resourcesPreset: "micro"
    ports:
      http: 3000
    ingress:
      - huginn
    vpn: {}
    # postgresql bitnami subchart
    # https://artifacthub.io/packages/helm/bitnami/postgresql#parameters
    postgresql:
      backup:
        enabled: true
        cronjob:
          schedule: "0 5 * * *"
          concurrencyPolicy: Replace
          storage:
            # disable PVC, we'll save backups to a local dir
            enabled: false
            mountPath: /backup/pgdump
          extraVolumes:
            - name: backup-dir
              hostPath:
                path: /opt/huginn/postgresql/backup
                type: Directory
          extraVolumeMounts:
            - name: backup-dir
              mountPath: /backup/pgdump
    ## App secret token
    appSecretToken:
      ## Specify the secret containing the App secret token
      secretName: huginn-app-secret-token
      secretKey: app-secret-token
    ## Invitation code that users must submit before registration
    ##
    ## Normally not needed, since your Huginn instance is protected by Authentik
    invitationCode:
      ## Whether invitation code is required
      enabled: false
      ## Specify the secret containing the Invitation code
      secretName: ""
      secretKey: ""
    ## Whether to require email confirmation for new users
    ##
    ## Normally not needed, since your Huginn instance is protected by Authentik
    requireConfirmedEmail: false
    # Specify the default User-Agent header value for HTTP requests made
    userAgent: "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.3"
    # see all the available env vars
    # https://github.com/huginn/huginn/blob/master/.env.example
    extraEnv: []
  playwright:
    enabled: false
    exposed: false
    name: playwright
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/browserless/chromium
      tag: v2.36.0
      pullPolicy: IfNotPresent
    securityContext:
      strict: false
    resourcesPreset: "micro"
    ports:
      http: 3000
    ingress: []
    token: "" # REQUIRED
    timeout: 60000
    maxConcurrentSessions: 4
    maxQueueLength: 10
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/playwright/config
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
    vpn: {}
    # see the full list of supported env vars
    # https://docs.browserless.io/docker/config
    #
    extraEnv: []
  changedetectionio:
    enabled: false
    exposed: true
    name: changedetection
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/dgtlmoon/changedetection.io
      tag: "0.51"
      pullPolicy: Always
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 5000
    ingress:
      - changedetectionio
      - changedetection
    vpn: {}
    playwrightLaunchOptions:
      # see the full list of available launch options
      # https://docs.browserless.io/chrome-flags
      stealth: true
      headless: true
      args: ["--lang=en-US", "--hide-scrollbars"]
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/changedetectionio/config
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
    extraEnv: []
  archivebox:
    enabled: false
    exposed: true
    name: archivebox
    replicaCount: 1
    critical: false
    image:
      repository: archivebox/archivebox
      tag: "0.7"
      pullPolicy: Always
    securityContext:
      # uses its own mechanism for switching to a non-root user via PUID/GUID
      strict: false
    resourcesPreset: "none"
    ports:
      http: 8000
    ingress:
      - archivebox
      - archive
    vpn: {}
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: data
    #   hostPath:
    #     path: /opt/archivebox/data
    #     type: DirectoryOrCreate
    # mounts:
    #   data:
    #     volume: data
    persistence:
      volumes: []
      mounts:
        data:
          volume: ""
    adminCredentials:
      secretName: archivebox-admin-credentials
      usernameKey: username
      passwordKey: password
    mediaMaxSize: 750m
    timeout: 120
    saveTitle: true
    saveFavicon: true
    saveWget: false
    saveWarc: false
    savePdf: false
    saveScreenshot: false
    saveDom: false
    saveSinglefile: true
    saveReadability: true
    saveMercury: false
    saveGit: false
    saveMedia: false
    saveArchiveDotOrg: false
    userAgent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36"
    # see the full list of supported env vars
    # https://github.com/ArchiveBox/ArchiveBox/wiki/Configuration
    #
    extraEnv: []
  apprise:
    enabled: false
    exposed: true
    name: apprise
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/caronc/apprise
      tag: "1.2"
      pullPolicy: Always
    securityContext:
      # incompatible with read-only file system
      # though non-root user is supported
      strict: false
    resourcesPreset: "micro"
    ports:
      http: 8000
    ingress:
      - apprise
    logLevel: INFO
    statefulMode: simple
    # attach size is limited to 500 Mb
    attachSizeMegabytes: 500
    defaultConfigId: apprise
    defaultTheme: dark
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/apprise/config
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
    extraEnv: []
  gotify:
    enabled: false
    exposed: true
    name: gotify
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/gotify/server
      tag: "2.7"
      pullPolicy: Always
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 80
    ingress:
      - gotify
    vpn: {}
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: data
    #   hostPath:
    #     path: /opt/gotify/data
    #     type: DirectoryOrCreate
    # mounts:
    #   data:
    #     volume: data
    persistence:
      volumes: []
      mounts:
        data:
          volume: ""
    adminCredentials:
      secretName: gotify-admin-credentials
      usernameKey: username
      passwordKey: password
    databaseDialect: sqlite3
    databaseConnection: data/gotify.db
    # see the full list of supported env vars
    # https://gotify.net/docs/config#environment-variables
    #
    extraEnv: []
  kavita:
    enabled: false
    exposed: true
    name: kavita
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/kareadita/kavita
      tag: "0.8.8"
      pullPolicy: IfNotPresent
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 5000
    ingress:
      - kavita
    vpn: {}
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/kavita/config
    #     type: DirectoryOrCreate
    # - name: library
    #   hostPath:
    #     path: /data/library/books
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    #   library:
    #     volume: library
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
        library:
          volume: ""
    extraEnv: []
  stump:
    enabled: false
    exposed: true
    name: stump
    replicaCount: 1
    critical: false
    image:
      repository: aaronleopold/stump
      tag: "0.0.12"
      pullPolicy: IfNotPresent
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 10801
    ingress:
      - stump
    vpn: {}
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/stump/config
    #     type: DirectoryOrCreate
    # - name: library
    #   hostPath:
    #     path: /data/library/books
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    #   library:
    #     volume: library
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
        library:
          volume: ""
    extraEnv: []
  calibre:
    enabled: false
    exposed: true
    name: calibre
    replicaCount: 1
    critical: false
    image:
      repository: crocodilestick/calibre-web-automated
      tag: V3.1.4
      pullPolicy: IfNotPresent
    securityContext:
      # the image uses s6-overlay,
      # which has its own mechanism for switching to a non-root user via PUID/GUID
      strict: false
    resourcesPreset: "micro"
    ports:
      http: 8083
    ingress:
      - calibre
      - books
    vpn: {}
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/calibre-web-automated/config
    #     type: DirectoryOrCreate
    # - name: ingest
    #   hostPath:
    #     path: /opt/calibre-web-automated/ingest
    #     type: DirectoryOrCreate
    # - name: data
    #   hostPath:
    #     path: /data/library/books
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    #   ingest:
    #     volume: ingest
    #   data:
    #     volume: data
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
        ingest:
          volume: ""
        data:
          volume: ""
    extraEnv: []
  calibrebookdownloader:
    enabled: false
    exposed: true
    name: calibre-book-downloader
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/calibrain/calibre-web-automated-book-downloader
      tag: latest
      pullPolicy: Always
    securityContext:
      # has its own mechanism for switching to a non-root user via UID/UID
      strict: false
    resourcesPreset: "micro"
    ports:
      http: 8084
    ingress:
      - calibre-book-downloader
      - annas-archive
      - anna-archive
    # Anna's Archive is blocked in some countries, you can use a VPN to bypass the block
    vpn:
      enabled: false
    # bypasses Cloudflare's protection when downloading books from Anna's Archive
    useCfBypass: true
    # preferred books language, you can add more languages separated by a comma
    bookLanguage: "en,ru"
    # restricts the search to specific formats
    # you can add more formats separated by a comma
    #
    # pay attention to compatibility with CWA - some formats may not be ingested
    # the default value is synced with the current version of CWA (3.0.1)
    #
    # formats are sorted in the order of preference
    supportedFormats: "epub,azw,azw3,azw4,mobi,fb2,cbz,cbr,cb7,cbc,chm,pdf,djvu,docx,fbz,html,htmlz,lit,lrf,odt,prc,pdb,pml,rb,rtf,snb,tcr,txtz"
    annasArchiveDonatorKey: "" # set your donator key if you have one; this will make downloads much faster and more reliable
    extraEnv: []
  openbooks:
    enabled: false
    exposed: true
    name: openbooks
    replicaCount: 1
    critical: false
    image:
      repository: evanbuss/openbooks
      tag: "4.5.0"
      pullPolicy: IfNotPresent
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 8080
    ingress:
      - openbooks
      - irchighway
      - highway
    # your IRC name, it will be used to identify you in the IRC channel
    ircname: pebble
    # if true, won't send files to browser
    noBrowserDownloads: false
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: ingest
    #   hostPath:
    #     path: /opt/calibre-web-automated/ingest
    #     type: DirectoryOrCreate
    # mounts:
    #   # This mount MUST point to the same volume (and optionally subPath) as CWA ingest mount
    #   calibre-ingest:
    #     volume: ingest
    persistence:
      volumes: []
      mounts:
        calibre-ingest:
          volume: ""
  convertx:
    enabled: false
    exposed: true
    name: convertx
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/c4illin/convertx
      tag: v0.15.1
      pullPolicy: IfNotPresent
    securityContext:
      strict: true
    resourcesPreset: "none"
    ports:
      http: 3000
    ingress:
      - convertx
      - convert
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: data
    #   hostPath:
    #     path: /opt/convertx/data
    #     type: DirectoryOrCreate
    # mounts:
    #   data:
    #     volume: data
    persistence:
      volumes: []
      mounts:
        data:
          volume: ""
    autoDeleteEveryNHours: 24
    # see available env vars
    # https://github.com/C4illin/ConvertX?tab=readme-ov-file#environment-variables
    #
    extraEnv: []
  pinchflat:
    enabled: false
    exposed: true
    name: pinchflat
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/kieraneglin/pinchflat
      tag: v2025.6.6
      pullPolicy: IfNotPresent
    securityContext:
      strict: false
    resourcesPreset: "micro"
    ports:
      http: 8945
    ingress:
      - pinchflat
      - youtube
    vpn: {}
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/pinchflat/config
    #     type: DirectoryOrCreate
    # - name: data
    #   hostPath:
    #     path: /data/library/videos
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    #   data:
    #     volume: data
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
        data:
          volume: ""
    extraEnv:
      - name: LOG_LEVEL
        value: info
  mealie:
    enabled: false
    exposed: true
    name: mealie
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/mealie-recipes/mealie
      tag: v3.4.0
      pullPolicy: IfNotPresent
    securityContext:
      strict: true
    resourcesPreset: "none"
    ports:
      http: 9000
    ingress:
      - mealie
      - recipes
      - recipe
    ## Allow user sign-up without token
    ##
    allowSignup: false
    tokenTimeHours: 4320
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: data
    #   hostPath:
    #     path: /opt/mealie/data
    #     type: DirectoryOrCreate
    # mounts:
    #   data:
    #     volume: data
    persistence:
      volumes: []
      mounts:
        data:
          volume: ""
    extraEnv:
      ## Log Level
      ## critical, error, warning, info, debug
      ##
      - name: LOG_LEVEL
        value: "info"
  librechat:
    enabled: false
    exposed: true
    name: librechat
    replicaCount: 1
    critical: true
    image:
      repository: ghcr.io/danny-avila/librechat
      tag: v0.8.0
      pullPolicy: IfNotPresent
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 3080
    ingress:
      - librechat
      - chatgpt
      - ai
    secretName: librechat-secret
    endpoints: "openAI"
    allowEmailLogin: false
    allowSocialLogin: true
    # controls how long an auth session lives
    # 180 days (15552000000 milliseconds) by default
    #
    # this settings has no effect if you've enabled OpenID refresh tokens
    # see https://www.librechat.ai/docs/configuration/authentication/OAuth2-OIDC/token-reuse
    # when this feature is active, the refresh token is issued by your OpenID provider instead of LibreChat
    refreshTokenExpiryMilliseconds: 15552000000
    customFooter: ""
    # enable search backed by MeiliSearch
    # see https://www.librechat.ai/docs/features/search
    #
    # make sure the MeiliSearch service is enabled
    search: false
    # Persistence config
    #
    # Example:
    # # directory where to keep user-uploaded images
    # volumes:
    # - name: client-images
    #   hostPath:
    #     path: /opt/librechat/client/images
    #     type: DirectoryOrCreate
    # mounts:
    #   client-images:
    #     volume: client-images
    persistence:
      volumes: []
      mounts:
        client-images:
          volume: ""
    mongodb:
      backup:
        enabled: true
        cronjob:
          schedule: "0 5 * * *"
          concurrencyPolicy: Replace
          storage:
            # disable PVC, we'll save backups to a local dir
            enabled: false
            mountPath: /backup/mongodump
          extraVolumes:
            - name: backup-dir
              hostPath:
                path: /opt/librechat/postgresql/backup
                type: Directory
          extraVolumeMounts:
            - name: backup-dir
              mountPath: /backup/mongodump
      systemLogVerbosity: 0
    extraEnvFromSecret: ""
    # you can pass your API keys and other secrets securely here
    extraEnvSecrets: []
    # - name: OPENAI_API_KEY
    #   secretName: librechat-endpoints-secret
    #   secretKey: librechat-openai-api-key
    # - name: OPENROUTER_KEY
    #   secretName: librechat-endpoints-secret
    #   secretKey: librechat-openrouter-api-key
    extraEnv:
      # debug logging
      - name: DEBUG_LOGGING
        value: "true"
      - name: DEBUG_CONSOLE
        value: "true"
        # json format is preffered if you the Loki/Grafana stack built-in to this chart
      - name: CONSOLE_JSON
        value: "false"
  meilisearch:
    enabled: false
    exposed: false
    name: meilisearch
    replicaCount: 1
    critical: true
    image:
      repository: getmeili/meilisearch
      tag: v1.23.0
      pullPolicy: Always
    securityContext:
      strict: true
    resourcesPreset: "medium"
    ports:
      http: 7700
    ingress: []
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: data
    #   hostPath:
    #     path: /opt/meilisearch/data
    #     type: DirectoryOrCreate
    # mounts:
    #   data:
    #     volume: data
    persistence:
      volumes: []
      mounts:
        data:
          volume: ""
    masterKey:
      secretName: meilisearch-secret
      secretKey: master-key
    ## Upgrade tool configuration
    ##
    ## Usage guide:
    ## 1. Set upgrade.enabled to true
    ## 2. Set image tag to the version you had prior to the chart update
    ## 3. Set action to "export" and deploy the chart
    ##   - Wait for the export job to complete
    ##   - Verify the dump created: `kubectl logs job meilisearch-upgrade-export`
    ## 4. Scale down the MeiliSearch: `kubectl scale deployment meilisearch --replicas 0`
    ## 5. Set action to "import" and deploy the chart
    ##   - Verify the dump imported: `kubectl logs deployment meilisearch`
    ##   - The existing database is automatically backed up during import to `data.ms.old`
    ## 6. Set upgrade.enabled to false and deploy the chart. The upgrade procees is finished!
    ##
    upgrade:
      ## Whether upgrade process is in progress
      enabled: false
      ## Action
      ##
      action: "export"
      ## PVC used to store dumps required for upgrade
      ##
      pvc:
        size: "1Gi"
        storageClass: ""
      ## Affinity configuration for the export job
      ##
      affinity: {}
    extraEnv: []
  obsidianLivesync:
    enabled: false
    exposed: true
    name: obsidian-livesync
    replicaCount: 1
    critical: true
    image:
      repository: couchdb
      tag: "3.5"
      pullPolicy: IfNotPresent
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports:
      http: 5984
    ingress:
      - obsidian-livesync
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: data
    #   hostPath:
    #     path: /opt/obsidian-livesync/data
    #     type: DirectoryOrCreate
    # - name: etc-local-d
    #   hostPath:
    #     path: /opt/obsidian-livesync/etc/local.d
    #     type: DirectoryOrCreate
    # mounts:
    #   data:
    #     volume: data
    #   etc-local-d:
    #     volume: etc-local-d
    persistence:
      volumes: []
      mounts:
        data:
          volume: ""
        etc-local-d:
          volume: ""
    adminCredentials:
      secretName: obsidian-livesync-admin-credentials
      usernameKey: username
      passwordKey: password
    ## Optional secret holding the CouchDB cookie auth secret (COUCHDB_SECRET)
    ##
    cookieAuthSecret:
      secretName: obsidian-livesync-cookie-secret
      secretKey: secret
    ## Custom CouchDB configuration
    ## See https://docs.couchdb.org/en/stable/config/intro.html
    ##
    configuration: |
      [couchdb]
      single_node = true
      max_document_size = 50000000

      [httpd]
      WWW-Authenticate = Basic realm="couchdb"

      [chttpd]
      bind_address = 0.0.0.0
      port = 5984
      ;; Allow only the listed options to be changed via the HTTP Config API.
      ;; Other options require modifying this config directly.
      config_whitelist = [{chttpd,config_whitelist}, {log,level}, {etc,etc}]
      require_valid_user = false
      require_valid_user_except_for_up = true
      max_http_request_size = 4294967296 ; 4 GB
      enable_cors = true

      [chttpd_auth]

      [cors]
      credentials = true
      origins = app://obsidian.md,capacitor://localhost,http://localhost
    vpn: {}
    extraEnv: []
  stash:
    enabled: false
    exposed: true
    name: stash
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/hotio/stash
      tag: release-0.28.1
      pullPolicy: IfNotPresent
    securityContext:
      strict: false
    resourcesPreset: "small"
    ports:
      http: 9999
    ingress:
      - stash
    vpn: {}
    # Persistence config
    #
    # Example:
    # ## Point this at your collection
    # volumes:
    # - name: data
    #   hostPath:
    #     path: /data/library-xxx/xxx
    #     type: DirectoryOrCreate
    # - name: config
    #   hostPath:
    #     path: /opt/stash/.stash
    #     type: DirectoryOrCreate
    # - name: metadata
    #   hostPath:
    #     path: /opt/stash/metadata
    #     type: DirectoryOrCreate
    # - name: cache
    #   hostPath:
    #     path: /opt/stash/cache
    #     type: DirectoryOrCreate
    # - name: blobs
    #   hostPath:
    #     path: /opt/stash/blobs
    #     type: DirectoryOrCreate
    # - name: generated
    #   hostPath:
    #     path: /opt/stash/generated
    #     type: DirectoryOrCreate
    # mounts:
    #   data:
    #     volume: data
    #   config:
    #     volume: config
    #   metadata:
    #     volume: metadata
    #   cache:
    #     volume: cache
    #   blobs:
    #     volume: blobs
    #   generated:
    #     volume: generated
    persistence:
      volumes: []
      mounts:
        data:
          volume: ""
        config:
          volume: ""
        metadata:
          volume: ""
        cache:
          volume: ""
        blobs:
          volume: ""
        generated:
          volume: ""
    extraEnv: []
  opengist:
    enabled: false
    exposed: true
    name: opengist
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/thomiceli/opengist
      tag: "1.11"
      pullPolicy: Always
    securityContext:
      strict: false
    resourcesPreset: "micro"
    ports:
      http: 6157
    ingress:
      - opengist
      - gist
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: config
    #   hostPath:
    #     path: /opt/opengist/config
    #     type: DirectoryOrCreate
    # mounts:
    #   config:
    #     volume: config
    persistence:
      volumes: []
      mounts:
        config:
          volume: ""
    vpn: {}
    # see the full list of supported env vars
    # https://opengist.io/docs/configuration/cheat-sheet.html
    extraEnvSecrets: []
    extraEnv:
      - name: OG_LOG_LEVEL
        value: "warn"
  isponsorblocktv:
    enabled: false
    exposed: false
    name: isponsorblocktv
    replicaCount: 1
    critical: false
    image:
      repository: ghcr.io/dmunozv04/isponsorblocktv
      tag: main
      pullPolicy: Always
    securityContext:
      strict: true
    resourcesPreset: "micro"
    ports: {}
    ingress: []
    # Persistence config
    #
    # Example:
    # volumes:
    # - name: data
    #   hostPath:
    #     path: /opt/isponsorblocktv/
    #     type: DirectoryOrCreate
    # mounts:
    #   data:
    #     volume: data
    persistence:
      volumes: []
      mounts:
        data:
          volume: ""
    vpn: {}
    extraEnv: []
